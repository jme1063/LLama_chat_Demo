# LLM Checker

A simple FastAPI web app for chatting with a locally hosted Llama LLM and asking questions about uploaded PDF files.

## Features
- Chat with Llama 3 (via Ollama) in your browser
- Upload a PDF and ask questions about its content
- Syllabus compliance checker endpoint

## Requirements
- Python 3.8+
- [Ollama](https://ollama.com/) installed and running locally
- Llama 3 model available in Ollama (e.g., `ollama run llama3` works in your terminal)
- Python packages:
  - fastapi
  - uvicorn
  - pdfplumber
  - python-multipart

## Installation
1. Clone or download this repository.
2. Install the required Python packages:
   ```sh
   pip install fastapi uvicorn pdfplumber python-multipart
   ```
3. Make sure Ollama is running and the Llama 3 model is available locally.

## Usage
1. Start the FastAPI server:
   ```sh
   python llm_checker.py
   ```
2. Open your browser and go to [http://localhost:8000/](http://localhost:8000/)
3. Chat with Llama or upload a PDF and ask questions about its content.

## Endpoints
- `/` : Web chat interface
- `/chat` : Chat API (GET/POST)
- `/upload_pdf` : Upload a PDF file (POST, multipart/form-data)
- `/check_syllabus` : Syllabus compliance checker (GET/POST)

## Notes
- All LLM responses are generated by your local Ollama instance using the Llama 3 model.
- Depending on hardware, the responses will take some time to be produced. as long as you see OK in the terminal here, you can verify it's connected and responding.
- Uploaded PDF content is stored in memory for the current session only.
- This is a demo app and not intended for production use.

## License
MIT License
